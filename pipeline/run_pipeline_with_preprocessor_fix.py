#!/usr/bin/env python3
"""
Complete C++ FIM Dataset Generation and Filtering Pipeline - Fixed Version

This script provides a complete end-to-end pipeline with preprocessor structure fix:
0. Phase 0 (Optional): Generate dataset with preprocessor structure validation
1. Phase 1: Quality filtering based on prefix context patterns
2. Phase 2: Middle content quality filtering  
3. Phase 3B: Length-based adaptive filtering (improved version)

The key improvement: Preprocessor structure validation prevents creation of
invalid contexts like "#else" without corresponding "#ifdef".

Usage: 
  python run_pipeline_with_preprocessor_fix.py [--regenerate] [--verbose]
  
Options:
  --regenerate      Regenerate the base dataset with preprocessor fixes
  --verbose         Show detailed output from each phase
"""

import subprocess
import sys
import os
import argparse
import time
import json
import re
from pathlib import Path

def check_file_exists(filepath, description=""):
    """Check if required input file exists"""
    if not os.path.exists(filepath):
        print(f"ERROR: Required file '{filepath}' not found")
        if description:
            print(f"       {description}")
        return False
    return True

def validate_preprocessor_fix(filepath, sample_size=1000):
    """Validate that the preprocessor fix worked correctly"""
    print(f"\n=== Validating Preprocessor Fix in {os.path.basename(filepath)} ===")
    
    orphaned_else_count = 0
    total_checked = 0
    
    with open(filepath, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f):
            if total_checked >= sample_size:
                break
                
            if not line.strip():
                continue
                
            total_checked += 1
            entry = json.loads(line)
            content = entry.get('content', '')
            
            # Extract prefix from FIM format
            prefix_match = re.search(r'<fim_prefix>(.*?)<fim_suffix>', content, re.DOTALL)
            if prefix_match:
                prefix = prefix_match.group(1)
                lines = prefix.split('\n')
                
                # Check for #else without corresponding #if/#ifdef
                has_if_directive = any(
                    '#ifdef' in line or '#ifndef' in line or 
                    re.match(r'#\s*if\b', line.strip()) 
                    for line in lines
                )
                has_else = any('#else' in line for line in lines)
                
                if has_else and not has_if_directive:
                    orphaned_else_count += 1
                    if orphaned_else_count <= 3:  # Show first 3 examples
                        print(f"\n⚠️  Found orphaned #else at line {line_num + 1}:")
                        print(f"Prefix: {prefix[:150]}...")
    
    print(f"\nValidation Results:")
    print(f"  Samples checked: {total_checked}")
    print(f"  Orphaned #else patterns: {orphaned_else_count}")
    
    if orphaned_else_count == 0:
        print(f"  ✅ SUCCESS: No orphaned #else patterns found!")
        return True
    else:
        print(f"  ❌ ISSUE: {orphaned_else_count} orphaned #else patterns found")
        return False

def run_phase_0():
    """Run Phase 0: Generate base dataset with preprocessor fixes"""
    print("\n=== Phase 0: Generate Base Dataset (with Preprocessor Fixes) ===")
    
    # Check if code directory exists (look in parent directory)
    code_dir = os.path.exists('code') or os.path.exists('../code')
    if not code_dir:
        print("ERROR: 'code' directory not found. This phase requires C++ source files in a 'code/' directory")
        print("       Expected location: './code/' or '../code/'")
        return False
    
    print("Extracting pairs from C++ codebase with preprocessor structure validation...")
    start_time = time.time()
    
    try:
        result = subprocess.run([
            sys.executable, "extract_nextline_pairs.py"
        ], check=True, capture_output=True, text=True)
        
        if result.stdout:
            print(result.stdout.strip())
        
        elapsed = time.time() - start_time
        print(f"Phase 0 complete in {elapsed:.1f}s: prompts_codebricks_autogenerated_underscore.jsonl")
        
        # Validate the preprocessor fix
        if check_file_exists("prompts_codebricks_autogenerated_underscore.jsonl"):
            success = validate_preprocessor_fix("prompts_codebricks_autogenerated_underscore.jsonl")
            return success
            
        return False
        
    except subprocess.CalledProcessError as e:
        print(f"ERROR: Phase 0 failed with exit code {e.returncode}")
        if e.stderr:
            print(f"Error output: {e.stderr}")
        return False

def run_phase_1():
    """Run Phase 1: Prefix context quality filtering"""
    print("\n=== Phase 1: Prefix Context Quality Filtering ===")
    
    # Check input file
    if not check_file_exists("prompts_codebricks_autogenerated_underscore.jsonl", 
                            "Generated by Phase 0"):
        return False
    
    print("Applying prefix context quality filters...")
    start_time = time.time()
    
    try:
        result = subprocess.run([
            sys.executable, "filter_quality_fim_tasks.py"
        ], check=True, capture_output=True, text=True)
        
        if result.stdout:
            print(result.stdout.strip())
        
        elapsed = time.time() - start_time
        print(f"Phase 1 complete in {elapsed:.1f}s: prompts_codebricks_filtered_improved.jsonl")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"ERROR: Phase 1 failed with exit code {e.returncode}")
        if e.stderr:
            print(f"Error output: {e.stderr}")
        return False

def run_phase_2():
    """Run Phase 2: Middle content quality filtering"""
    print("\n=== Phase 2: Middle Content Quality Filtering ===")
    
    # Check input file
    if not check_file_exists("prompts_codebricks_filtered_improved.jsonl", 
                            "Generated by Phase 1"):
        return False
    
    print("Applying middle content quality filters...")
    start_time = time.time()
    
    try:
        result = subprocess.run([
            sys.executable, "apply_middle_content_filter.py"
        ], check=True, capture_output=True, text=True)
        
        if result.stdout:
            print(result.stdout.strip())
            
        elapsed = time.time() - start_time
        print(f"Phase 2 complete in {elapsed:.1f}s: prompts_codebricks_filtered_middle_quality.jsonl")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"ERROR: Phase 2 failed with exit code {e.returncode}")
        if e.stderr:
            print(f"Error output: {e.stderr}")
        return False

def run_phase_3b():
    """Run Phase 3B: Length-based adaptive filtering"""
    print("\n=== Phase 3B: Length-Based Adaptive Filtering ===")
    
    # Check input file
    if not check_file_exists("prompts_codebricks_filtered_middle_quality.jsonl", 
                            "Generated by Phase 2"):
        return False
    
    print("Applying length-based adaptive filters...")
    start_time = time.time()
    
    try:
        result = subprocess.run([
            sys.executable, "apply_phase3b_improved_filter.py"
        ], check=True, capture_output=True, text=True)
        
        if result.stdout:
            print(result.stdout.strip())
            
        elapsed = time.time() - start_time
        print(f"Phase 3B complete in {elapsed:.1f}s: prompts_codebricks_filtered_phase3b_improved.jsonl")
        
        # Validate final output
        if check_file_exists("prompts_codebricks_filtered_phase3b_improved.jsonl"):
            validate_preprocessor_fix("prompts_codebricks_filtered_phase3b_improved.jsonl", 2000)
        
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"ERROR: Phase 3B failed with exit code {e.returncode}")
        if e.stderr:
            print(f"Error output: {e.stderr}")
        return False

def main():
    parser = argparse.ArgumentParser(description="C++ FIM Pipeline with Preprocessor Structure Fix")
    parser.add_argument("--regenerate", action="store_true", 
                       help="Regenerate base dataset with preprocessor fixes")
    parser.add_argument("--verbose", action="store_true", 
                       help="Show detailed output from each phase")
    
    args = parser.parse_args()
    
    print("C++ FIM Dataset Pipeline with Preprocessor Structure Fix")
    print("=" * 60)
    print("Key Fix: Prevents #else without #ifdef in completion contexts")
    print("=" * 60)
    
    phases_run = 0
    total_start_time = time.time()
    
    # Phase 0: Generate pairs (optional)
    if args.regenerate:
        if not run_phase_0():
            print("\nPipeline stopped due to Phase 0 failure")
            sys.exit(1)
        phases_run += 1
    else:
        # Validate existing dataset
        if check_file_exists("prompts_codebricks_autogenerated_underscore.jsonl"):
            print("\nValidating existing base dataset...")
            validate_preprocessor_fix("prompts_codebricks_autogenerated_underscore.jsonl", 1000)
    
    # Phase 1: Quality filtering
    if not run_phase_1():
        print("\nPipeline stopped due to Phase 1 failure")
        sys.exit(1)
    phases_run += 1
    
    # Phase 2: Middle content filtering
    if not run_phase_2():
        print("\nPipeline stopped due to Phase 2 failure")
        sys.exit(1)
    phases_run += 1
    
    # Phase 3B: Length-based filtering
    if not run_phase_3b():
        print("\nPipeline stopped due to Phase 3B failure")
        sys.exit(1)
    phases_run += 1
    
    # Summary
    total_elapsed = time.time() - total_start_time
    print(f"\n{'=' * 60}")
    print("🎉 PIPELINE COMPLETED SUCCESSFULLY!")
    print(f"Phases run: {phases_run}")
    print(f"Total time: {total_elapsed:.1f}s")
    print("🔧 Key Improvement: Preprocessor structure validation added")
    
    # Show final output file info
    final_output = "prompts_codebricks_filtered_phase3b_improved.jsonl"
    if os.path.exists(final_output):
        file_size = os.path.getsize(final_output)
        print(f"📄 Final output: {final_output} ({file_size:,} bytes)")
        
        # Count lines in final output
        try:
            with open(final_output, 'r', encoding='utf-8') as f:
                line_count = sum(1 for _ in f)
            print(f"📊 Final dataset size: {line_count:,} examples")
        except Exception as e:
            print(f"Could not count lines in final output: {e}")
    
    print("\n✅ The dataset now includes preprocessor structure validation!")
    print("✅ No more orphaned #else patterns in completion contexts!")

if __name__ == "__main__":
    main()
