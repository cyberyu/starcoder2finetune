{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17000453-40c1-4478-aeaa-8ea4655a53dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syu2/anaconda3/envs/refact3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:15<00:00,  5.01s/it]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Hypthen Tag-----------------\n",
      "<fim-prefix>def fib(n):<fim-suffix>    else:\n",
      "        return fib(n - 2) + fib(n - 1)<fim-middle>\n",
      "\n",
      "<fim-prefix>def fib(n):<fim-suffix>    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n - 2) + fib(n - 1)<fim-middle>\n",
      "\n",
      "<fim-prefix>def fib(n):<fim-suffix>   \n",
      "--------------Under Score Tag-----------------\n",
      "<fim_prefix>def fib(n):<fim_suffix>    else:\n",
      "        return fib(n - 2) + fib(n - 1)<fim_middle>\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "<file_sep><fim_prefix><fim_suffix> 1000000007\n",
      "\n",
      "def main():\n",
      "    n = int(input())\n",
      "    print(fib(n))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()<fim_middle>/fibonacci.py\n",
      "def fib(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "       \n"
     ]
    }
   ],
   "source": [
    "# load the base and fine-tuned model\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import transformers\n",
    "from accelerate import PartialState\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    logging,\n",
    "    set_seed,\n",
    ")\n",
    "#from trl import SFTTrainer\n",
    "#from peft import PeftModel, PeftConfig\n",
    "# verify the load\n",
    "\n",
    "#merged_model_path= f\"/usr/project/llm-compressor/starcoder2_7b_22k_ft_80EM-quantized.w8a8_optimal\"\n",
    "\n",
    "merged_model_path = 'bigcode/starcoder2-7b'\n",
    "\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_model_path,\n",
    "    device_map=\"cuda:0\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
    "\n",
    "def generate_completion(text: str):\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    #print(len(inputs[0]))\n",
    "    outputs = model.generate(inputs, max_length=128, pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(outputs[0])\n",
    "\n",
    "\n",
    "\n",
    "print ('--------------Hyphen Tag-----------------')\n",
    "text = \"<fim-prefix>def fib(n):<fim-suffix>    else:\\n        return fib(n - 2) + fib(n - 1)<fim-middle>\"\n",
    "print(generate_completion(text))\n",
    "\n",
    "print ('--------------Under Score Tag-----------------')\n",
    "text_underscore= \"<fim_prefix>def fib(n):<fim_suffix>    else:\\n        return fib(n - 2) + fib(n - 1)<fim_middle>\"\n",
    "print(generate_completion(text_underscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47be1c6-20f9-4b29-a6c6-92448fcc2bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
